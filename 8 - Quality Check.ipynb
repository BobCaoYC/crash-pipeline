{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136c1228",
   "metadata": {},
   "source": [
    "\n",
    "# **ðŸ§ª Quality Check (QC)**\n",
    "\n",
    "Validates **Crash_type (binary or multi-class)**, **Hit and Run**, and **Fatality Risk**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c8f5a",
   "metadata": {},
   "source": [
    "## **0) (Optional) Install pandas/numpy (usually preinstalled)**\n",
    "\n",
    "Installs any missing Python packages so the notebook runs consistently on fresh environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179842f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e56b46",
   "metadata": {},
   "source": [
    "## **1) Configure outcome and thresholds**\n",
    "\n",
    "To know your DB, schema, table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffc282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "con.execute(\"ATTACH 'gold.duckdb' AS gold (READ_ONLY)\")\n",
    "\n",
    "# === Show all tables (DB, schema, table name) ===\n",
    "result = con.execute(\"\"\"\n",
    "  SELECT database_name, schema_name, table_name\n",
    "  FROM duckdb_tables()\n",
    "  WHERE database_name = 'gold'\n",
    "  ORDER BY 1,2,3\n",
    "\"\"\").df()\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4dd2a",
   "metadata": {},
   "source": [
    "Using the above details update the TABLE_BY_OUTCOME variable \n",
    "\n",
    "Sets the OUTCOME, connects table names to DuckDB, chooses the target column, and defines QC thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os, re, math\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Config ---\n",
    "OUTCOME = \"Crash_type\"  # or: \"Crash_type\", \"Hit and Run\", \"Fatality Risk\"\n",
    "DB_PATH = \"gold.duckdb\"\n",
    "TABLE_BY_OUTCOME = {\n",
    "    \"Crash_type\": 'gold\".\"main\".\"gold_crash_type',        # db.schema.Table_name â†’ gold.duckdb.gold.gold_crash_type\n",
    "    \"Hit and Run\": 'gold\".\"gold\".\"crashes',               # db.schema.Table_name â†’ gold.duckdb.gold.crashes \n",
    "    \"Fatality Risk\": 'gold\".\"gold\".\"gold_fatality_risk',  # db.schema.Table_name â†’ gold.duckdb.gold.gold_fatality_risk\n",
    "}\n",
    "\n",
    "\n",
    "TABLE_NAME = TABLE_BY_OUTCOME[OUTCOME]\n",
    "\n",
    "# --- Columns and Targets ---\n",
    "DEFAULT_TARGET_BY_OUTCOME = {\n",
    "    \"Crash_type\": \"crash_type\",\n",
    "    \"Hit and Run\": \"hit_and_run_i\",\n",
    "    \"Fatality Risk\": \"fatality_risk\"\n",
    "}\n",
    "TARGET_COL = None\n",
    "ID_COL = \"crash_record_id\"\n",
    "DATE_COL = \"crash_date\"\n",
    "\n",
    "# --- Leakage Hints & Thresholds ---\n",
    "LEAKAGE_HINTS = {\n",
    "    \"Hit and Run\": [\"hit_and_run\", \"hit-run\", \"hitrun\"],\n",
    "    \"Fatality Risk\": [\"injuries_fatal\", \"injuries_total\", \"injuries_incapacitating\"],\n",
    "    \"Crash_type\": [\"crash_type\"]\n",
    "}\n",
    "# QC thresholds (tune for your dataset size & tolerance)\n",
    "MISSING_WARN = 0.20   # warn if a column's missing-rate > 20%.\n",
    "                      # Typical: 0.10 (strict) to 0.30 (lenient). Use ~0.20 for general QC.\n",
    "\n",
    "HIGH_CARD_LIMIT = 100 # flag categorical columns with >100 unique values as \"high-cardinality\".\n",
    "                      # Typical: 50â€“200. Lower for small datasets; higher if you expect many IDs/buckets.\n",
    "\n",
    "OUTLIER_RATE_WARN = 0.02  # warn if an individual numeric column has >2% outliers by IQR rule.\n",
    "                          # Typical: 0.01 (strict) to 0.05 (lenient). Use ~0.02 for balanced sensitivity.\n",
    "\n",
    "\n",
    "# --- Derived Vars ---\n",
    "TARGET = (TARGET_COL or DEFAULT_TARGET_BY_OUTCOME.get(OUTCOME))\n",
    "assert OUTCOME in DEFAULT_TARGET_BY_OUTCOME, f\"Unsupported OUTCOME: {OUTCOME}\"\n",
    "print(f\"Outcome = {OUTCOME}  |  Target column = {TARGET}\")\n",
    "\n",
    "# --- Load data from DuckDB ---\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    df = con.execute(f'SELECT * FROM \"{TABLE_NAME}\"').df()\n",
    "\n",
    "print(f\"Loaded {TABLE_NAME} with shape: {df.shape}\")\n",
    "\n",
    "# --- Date conversion ---\n",
    "if DATE_COL in df.columns:\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "# --- Display sample ---\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da5a9d",
   "metadata": {},
   "source": [
    "## **2) Schema & primary key checks**\n",
    "\n",
    "Confirms required columns exist and verifies crash_record_id is non-null and unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "issues = issues if 'issues' in globals() else []\n",
    "\n",
    "REQUIRED_COLUMNS = [\n",
    "    'crash_record_id', 'weather_condition','lighting_condition'\n",
    "]\n",
    "\n",
    "missing_required = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "if missing_required:\n",
    "    issues.append((\"SCHEMA\", f\"Missing required columns: {missing_required}\"))\n",
    "    print(\"[SCHEMA] Missing:\", missing_required)\n",
    "\n",
    "if 'crash_record_id' not in df.columns:\n",
    "    issues.append((\"KEY\", \"crash_record_id column missing\"))\n",
    "else:\n",
    "    null_ids = df['crash_record_id'].isna().sum()\n",
    "    if null_ids > 0:\n",
    "        issues.append((\"KEY\", f\"{null_ids} rows have NULL crash_record_id\"))\n",
    "        print(\"[KEY] NULL crash_record_id rows:\", null_ids)\n",
    "    nunique_ids = df['crash_record_id'].nunique(dropna=True)\n",
    "    if nunique_ids != len(df):\n",
    "        approx_dupes = int(len(df) - nunique_ids)\n",
    "        issues.append((\"KEY\", f\"crash_record_id not unique (duplicates ~ {approx_dupes})\"))\n",
    "        print(\"[KEY] Non-unique crash_record_id; ~dupes:\", approx_dupes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b216a",
   "metadata": {},
   "source": [
    "## **3) QC readiness (no cleaning)**\n",
    "\n",
    "Validates basic dataframe readiness for QC (consistent names/types, safe coercions) â€” does not perform cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns = [re.sub(r\"\\s+\", \"_\", c.strip().lower()) for c in df.columns]\n",
    "ID = ID_COL.lower() if ID_COL else None\n",
    "DATE = DATE_COL.lower() if DATE_COL else None\n",
    "TARGET = TARGET.lower() if TARGET else None\n",
    "print(\"Target present?\", TARGET in df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136efa3b",
   "metadata": {},
   "source": [
    "## **4) Duplicates**\n",
    "\n",
    "Checks dataset grain and flags duplicate records or non-unique keys for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee67b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "issues = []\n",
    "if ID in df.columns:\n",
    "    dup_ids = df[ID].value_counts()\n",
    "    dup_ids = dup_ids[dup_ids > 1]\n",
    "    if not dup_ids.empty:\n",
    "        issues.append((\"DUPLICATES\", f\"{dup_ids.shape[0]} duplicated {ID} values\"))\n",
    "        display(dup_ids.head(10).to_frame(\"count\"))\n",
    "else:\n",
    "    before = df.shape[0]\n",
    "    after = df.drop_duplicates().shape[0]\n",
    "    if before - after > 0:\n",
    "        issues.append((\"DUPLICATES\", f\"{before - after} fully-duplicate rows\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318acfd",
   "metadata": {},
   "source": [
    "## **5) Missingness**\n",
    "\n",
    "Computes per-column missing rates and highlights columns exceeding the warning threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae287728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "miss = (df.isna().mean().rename(\"missing_rate\").reset_index()\n",
    "          .rename(columns={\"index\":\"column\"})\n",
    "          .sort_values(\"missing_rate\", ascending=False))\n",
    "display(miss.head(20))\n",
    "if (miss[\"missing_rate\"] > MISSING_WARN).any():\n",
    "    issues.append((\"MISSINGNESS\", \"Some columns exceed missingness threshold\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409955b6",
   "metadata": {},
   "source": [
    "## **6) Target checks** \n",
    "\n",
    "Profiles the target: coerces to binary/ordinal when applicable and stores distribution metadata for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "# Canonicalize OUTCOME names so students can use small variants\n",
    "_OUTCOME_ALIASES = {\n",
    "    \"crash_type\": \"Crash_type\",\n",
    "    \"crash type\": \"Crash_type\",\n",
    "    \"hit&run\": \"Hit and Run\",\n",
    "    \"hit & run\": \"Hit and Run\",\n",
    "    \"fatality_risk\": \"Fatality Risk\",\n",
    "    \"fatality  risk\": \"Fatality Risk\",\n",
    "}\n",
    "_outcome_key = OUTCOME.strip()\n",
    "_outcome_key = _OUTCOME_ALIASES.get(_outcome_key.lower(), _outcome_key)\n",
    "\n",
    "# Enforce allowed outcomes only\n",
    "_ALLOWED = {\"Crash_type\", \"Hit and Run\", \"Fatality Risk\"}\n",
    "if _outcome_key not in _ALLOWED:\n",
    "    raise ValueError(f\"Unsupported OUTCOME: {OUTCOME}. Allowed: {sorted(_ALLOWED)}\")\n",
    "OUTCOME = _outcome_key  # overwrite with canonical value\n",
    "\n",
    "# --- Helpers (generic) ---\n",
    "def _normalize_binary_series_generic(s: pd.Series) -> Tuple[pd.Series, Optional[Dict[str,int]]]:\n",
    "    \"\"\"\n",
    "    Coerce common binary encodings to {0,1}. (No KSI terms here.)\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return s, None\n",
    "    uniq_raw = set(pd.Series(s.dropna().unique()).tolist())\n",
    "    if uniq_raw <= {0,1}:\n",
    "        return s.astype(\"Int64\"), None\n",
    "\n",
    "    sv = s.astype(str).str.strip().str.lower()\n",
    "    mapping = {\n",
    "        \"1\": 1, \"0\": 0,\n",
    "        \"true\": 1, \"false\": 0,\n",
    "        \"y\": 1, \"n\": 0,\n",
    "        \"yes\": 1, \"no\": 0,\n",
    "    }\n",
    "    mapped = sv.map(mapping)\n",
    "    if mapped.dropna().isin([0,1]).all() and mapped.notna().any():\n",
    "        return mapped.astype(\"Int64\"), mapping\n",
    "    return s, None\n",
    "\n",
    "def _try_map_fatality_ordinal(s: pd.Series) -> Tuple[Optional[pd.Series], Optional[Dict[str,int]]]:\n",
    "    \"\"\"\n",
    "    Map common severity terms to an ordered 0..K-1 scale for Fatality Risk if present.\n",
    "    \"\"\"\n",
    "    sv = s.astype(str).str.strip().str.lower()\n",
    "    # If small numeric scale (0..5 etc.), just coerce\n",
    "    num_try = pd.to_numeric(sv, errors=\"coerce\")\n",
    "    uniq = sorted(num_try.dropna().unique())\n",
    "    if 2 < len(uniq) <= 8 and (pd.Series(uniq).max() <= 10):\n",
    "        return num_try.astype(\"Int64\"), {\"numeric_scale_detected\": True}\n",
    "    # Text buckets (customize as needed)\n",
    "    lut = {\n",
    "        \"none\": 0, \"no injury\": 0, \"no fatality\": 0,\n",
    "        \"low\": 1, \"minor\": 1,\n",
    "        \"medium\": 2, \"moderate\": 2, \"med\": 2,\n",
    "        \"high\": 3, \"serious\": 3, \"severe\": 3,\n",
    "        \"fatal\": 4, \"death\": 4\n",
    "    }\n",
    "    mapped = sv.map(lut)\n",
    "    if mapped.notna().any():\n",
    "        return mapped.astype(\"Int64\"), lut\n",
    "    return None, None\n",
    "\n",
    "# --- Target analysis starts here ---\n",
    "target_info = {}\n",
    "if TARGET and TARGET in df.columns:\n",
    "    col = TARGET\n",
    "    nonnull = df[col].dropna()\n",
    "    uniq_vals = sorted(pd.Series(nonnull.unique()).tolist(), key=lambda x: str(x))\n",
    "    k = len(uniq_vals)\n",
    "\n",
    "    target_info = {\n",
    "        \"name\": col,\n",
    "        \"k\": k,\n",
    "        \"values\": uniq_vals[:12]\n",
    "    }\n",
    "\n",
    "    expects_binary = False\n",
    "    if OUTCOME == \"Hit and Run\":\n",
    "        expects_binary = True\n",
    "    elif OUTCOME == \"Crash_type\":\n",
    "        # Crash_type can be binary (2 string labels) or multiclass\n",
    "        expects_binary = (k == 2)\n",
    "        if k > 30:\n",
    "            issues.append((\"TARGET\", f\"{k} categories â€” consider grouping rare levels for Crash_type\"))\n",
    "    elif OUTCOME == \"Fatality Risk\":\n",
    "        # Typically binary (KSI vs Non-KSI) or ordinal severity; try binary if k==2, else try ordinal\n",
    "        expects_binary = (k == 2)\n",
    "\n",
    "    series_for_balance = None\n",
    "\n",
    "    # ----- Outcome-specific handling -----\n",
    "    if OUTCOME == \"Fatality Risk\" and k > 2:\n",
    "        # Try ordinal mapping first\n",
    "        ord_series, ord_map = _try_map_fatality_ordinal(df[col])\n",
    "        if ord_series is not None:\n",
    "            ord_col = col + \"_ord\"\n",
    "            df[ord_col] = ord_series\n",
    "            target_info[\"ordinal_column\"] = ord_col\n",
    "            target_info[\"interpreted_as\"] = \"ordinal\"\n",
    "            target_info[\"ordinal_mapping\"] = ord_map\n",
    "        else:\n",
    "            # Try binary mapping using common KSI strings (scoped here only)\n",
    "            sv = df[col].astype(str).str.strip().str.lower()\n",
    "            ksi_map = {\n",
    "                \"ksi\": 1, \"k.s.i\": 1,\n",
    "                \"non-ksi\": 0, \"non ksi\": 0, \"non_ksi\": 0, \"non ksi \": 0\n",
    "            }\n",
    "            mapped = sv.map(ksi_map)\n",
    "            if mapped.dropna().isin([0,1]).all() and mapped.notna().any():\n",
    "                bin_col = col + \"_bin\"\n",
    "                df[bin_col] = mapped.astype(\"Int64\")\n",
    "                target_info[\"binary_column\"] = bin_col\n",
    "                target_info[\"interpreted_as\"] = \"binary\"\n",
    "                issues.append((\"TARGET\", f\"Normalized `{col}` to {{0,1}} using KSI mapping â†’ `{bin_col}`\"))\n",
    "                series_for_balance = df[bin_col]\n",
    "            else:\n",
    "                issues.append((\"TARGET\", f\"`{col}` has {k} distinct values â€” expected binary/ordinal for Fatality Risk\"))\n",
    "\n",
    "    # Binary normalization for outcomes that should be binary\n",
    "    if expects_binary and series_for_balance is None:\n",
    "        s_norm, used_map = _normalize_binary_series_generic(df[col])\n",
    "        uniq_after = set(pd.Series(s_norm.dropna().unique()).tolist())\n",
    "        if uniq_after <= {0,1}:\n",
    "            bin_col = col + \"_bin\"\n",
    "            df[bin_col] = s_norm\n",
    "            target_info[\"binary_column\"] = bin_col\n",
    "            target_info[\"interpreted_as\"] = \"binary\"\n",
    "            if used_map:\n",
    "                issues.append((\"TARGET\", f\"Normalized `{col}` to {{0,1}} using mapping {used_map} â†’ `{bin_col}`\"))\n",
    "            series_for_balance = df[bin_col]\n",
    "        else:\n",
    "            if k == 2:\n",
    "                issues.append((\"TARGET\", f\"`{col}` is binary but uses non 0/1 encodings: {uniq_vals}\"))\n",
    "            else:\n",
    "                # For Crash_type multiclass, we accept; for Fatality Risk we warned above.\n",
    "                pass\n",
    "\n",
    "    # Always store distribution (top 20)\n",
    "    dist = df[col].value_counts(dropna=False).to_frame(\"count\")\n",
    "    target_info[\"distribution\"] = dist.head(20)\n",
    "\n",
    "    # Balance metrics for binary targets (from series_for_balance if set)\n",
    "    if series_for_balance is not None:\n",
    "        counts = series_for_balance.value_counts(dropna=False)\n",
    "        total = int(counts.sum())\n",
    "        pos = int(counts.get(1, 0))\n",
    "        neg = int(counts.get(0, 0))\n",
    "        minority = min(pos, neg)\n",
    "        majority = max(pos, neg)\n",
    "        minority_rate = (minority / total) if total else 0.0\n",
    "        imbalance_ratio = (majority / minority) if minority > 0 else float(\"inf\")\n",
    "        target_info[\"class_balance\"] = {\n",
    "            \"total\": total,\n",
    "            \"pos\": pos,\n",
    "            \"neg\": neg,\n",
    "            \"minority_rate\": minority_rate,\n",
    "            \"imbalance_ratio\": imbalance_ratio\n",
    "        }\n",
    "        # keep your chosen threshold; here 10%\n",
    "        if minority_rate < 0.10:\n",
    "            issues.append((\"IMBALANCE\", f\"Minority class <10% (minority_rate={minority_rate:.3f}). Consider resampling/thresholding.\"))\n",
    "\n",
    "    # Optional: if ordinal Fatality Risk was mapped, show level frequencies too\n",
    "    if OUTCOME == \"Fatality Risk\" and target_info.get(\"ordinal_column\"):\n",
    "        ord_dist = df[target_info[\"ordinal_column\"]].value_counts(dropna=False).sort_index().to_frame(\"count\")\n",
    "        target_info[\"ordinal_distribution\"] = ord_dist\n",
    "\n",
    "    # Display summary for the notebook run\n",
    "    print(f\"### Target `{col}` (k={k})\")\n",
    "    display(target_info[\"distribution\"])\n",
    "    cb = target_info.get(\"class_balance\")\n",
    "    if isinstance(cb, dict):\n",
    "        print(\"\\nClass balance (binary):\")\n",
    "        print(f\"- total={cb['total']}, pos={cb['pos']}, neg={cb['neg']}\")\n",
    "        print(f\"- minority_rate={cb['minority_rate']:.3f}, imbalance_ratio={cb['imbalance_ratio']:.2f}\")\n",
    "    if OUTCOME == \"Fatality Risk\" and target_info.get(\"ordinal_distribution\") is not None:\n",
    "        print(\"\\nOrdinal distribution (mapped):\")\n",
    "        display(target_info[\"ordinal_distribution\"])\n",
    "\n",
    "else:\n",
    "    issues.append((\"TARGET\", f\"Target `{TARGET}` not found\"))\n",
    "    print(f\"[WARN] Target `{TARGET}` not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf52e8c",
   "metadata": {},
   "source": [
    "## **7) Class Imbalance Indicator**\n",
    "Reports class counts/percentages and flags severe imbalance when the minority rate is too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "issues = issues if 'issues' in globals() else []\n",
    "\n",
    "if TARGET and TARGET in df.columns:\n",
    "    vc = df[TARGET].value_counts(dropna=False)\n",
    "    vc_df = vc.rename_axis(\"class\").to_frame(\"count\")\n",
    "    vc_df[\"pct\"] = (vc_df[\"count\"] / len(df)).round(4)\n",
    "\n",
    "    # Show distribution once\n",
    "    try:\n",
    "        display(vc_df)\n",
    "    except Exception:\n",
    "        print(vc_df)\n",
    "\n",
    "    # Persist into target_info for final report\n",
    "    target_info = locals().get(\"target_info\", {})\n",
    "    target_info[\"name\"] = TARGET\n",
    "    target_info[\"k\"] = int(df[TARGET].nunique(dropna=True))\n",
    "    target_info[\"distribution\"] = vc_df.reset_index()\n",
    "\n",
    "    # Determine how many classes (ignoring NaN)\n",
    "    levels = [cls for cls in vc.index.tolist() if pd.notna(cls)]\n",
    "    n_classes = len(levels)\n",
    "\n",
    "    # Optional: allow a user-configured positive class label\n",
    "    POSITIVE_LABEL = locals().get(\"POSITIVE_LABEL\", None)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        # Treat as binary even if labels are strings\n",
    "        if POSITIVE_LABEL in levels:\n",
    "            pos_name = POSITIVE_LABEL\n",
    "            neg_name = levels[0] if levels[1] == POSITIVE_LABEL else levels[1]\n",
    "        else:\n",
    "            # pick minority as positive by default\n",
    "            pos_name = vc.idxmin()\n",
    "            neg_name = vc.idxmax()\n",
    "\n",
    "        pos = int(vc.get(pos_name, 0))\n",
    "        neg = int(vc.get(neg_name, 0))\n",
    "        total = int(len(df))\n",
    "        pos_rate = (pos / total) if total else 0.0\n",
    "\n",
    "        # flag imbalance if extreme (<20% or >80% positive)\n",
    "        if pos_rate < 0.20 or pos_rate > 0.80:\n",
    "            issues.append((\"IMBALANCE\", f\"Target '{TARGET}' imbalanced (positive='{pos_name}', rate={pos_rate:.3f})\"))\n",
    "            print(f\"[IMBALANCE] Binary target detected: positive='{pos_name}', rate={pos_rate:.3f}\")\n",
    "        else:\n",
    "            print(f\"[IMBALANCE] Binary target detected: positive='{pos_name}', rate={pos_rate:.3f} (looks OK)\")\n",
    "\n",
    "        # store detailed balance for final report\n",
    "        minority = min(pos, neg)\n",
    "        target_info[\"class_balance\"] = {\n",
    "            \"total\": total,\n",
    "            \"pos\": pos,\n",
    "            \"neg\": neg,\n",
    "            \"pos_label\": str(pos_name),\n",
    "            \"neg_label\": str(neg_name),\n",
    "            \"minority_rate\": (minority / total) if total else 0.0,\n",
    "            \"imbalance_ratio\": (max(pos, neg) / max(1, minority)) if minority else float(\"inf\"),\n",
    "        }\n",
    "\n",
    "    elif n_classes > 2:\n",
    "        # Multi-class: just show top classes (already displayed above)\n",
    "        print(\"[IMBALANCE] Multi-class target â€” review class distribution above.\")\n",
    "    else:\n",
    "        print(\"[IMBALANCE] Target has <2 non-null classes; nothing to assess.\")\n",
    "    \n",
    "    # write back\n",
    "    locals()[\"target_info\"] = target_info\n",
    "\n",
    "else:\n",
    "    print(\"[IMBALANCE] Skipped: TARGET not set or missing from df.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd191bc",
   "metadata": {},
   "source": [
    "## **8) Boolean Normalization Audit for _i Flags**\n",
    "\n",
    "Audits all *_i indicators and warns if any use encodings outside {0,1,NaN}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "issues = issues if 'issues' in globals() else []\n",
    "\n",
    "# Consider any column ending with \"_i\" as an indicator/boolean.\n",
    "flag_cols = [c for c in df.columns if c.endswith('_i')]\n",
    "\n",
    "bad_flags = {}\n",
    "for c in flag_cols:\n",
    "    # look only at observed, non-null values\n",
    "    uniq = set(pd.Series(df[c].dropna().unique()).tolist())\n",
    "    # allow only {0,1}; NaN is allowed separately\n",
    "    if not (uniq <= {0, 1}):\n",
    "        bad_flags[c] = sorted(list(uniq))[:12]  # sample first few values\n",
    "\n",
    "if bad_flags:\n",
    "    issues.append((\"BOOLEANS\", f\"Non-binary encodings in flags: {list(bad_flags.keys())}\"))\n",
    "    print(\"[BOOLEANS] Non {0,1} values detected in:\")\n",
    "    for k, v in bad_flags.items():\n",
    "        print(f\"  - {k}: examples -> {v}\")\n",
    "else:\n",
    "    print(\"[BOOLEANS] All *_i flags are in {{0,1,NaN}}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fc445",
   "metadata": {},
   "source": [
    "## **9) Leakage** \n",
    "\n",
    "Surfaces name-based suspects and enforces removal of forbidden leakage/ID columns before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "issues = issues if 'issues' in globals() else []\n",
    "\n",
    "# 0) Outcome-specific name hints \n",
    "if TARGET and TARGET in df.columns:\n",
    "    hints = [h.lower() for h in LEAKAGE_HINTS.get(OUTCOME, [])]\n",
    "    found_hint = [c for c in df.columns if any(h in c.lower() for h in hints) and c != TARGET]\n",
    "    if found_hint:\n",
    "        issues.append((\"LEAKAGE\", f\"Suspected leakage columns (name hints): {found_hint[:12]}{'...' if len(found_hint)>12 else ''}\"))\n",
    "        display(pd.DataFrame({'suspected_leakage_cols': found_hint}).assign(source=\"hints\"))\n",
    "\n",
    "# 1) Expanded common leakage/ID/post-event list (flag only)\n",
    "COMMON_LEAKAGE_EXACT = {\n",
    "    # already had some of these\n",
    "    \"report_type\", \"photos_taken_i\", \"statements_taken_i\", \"date_police_notified\",\n",
    "    \"veh_vehicle_id_list_json\", \"ppl_person_id_list_json\", \"location_json\", \"street_name\",\n",
    "    # more admin/ID/meta\n",
    "    \"rd_no\", \"report_number\", \"case_no\", \"case_number\", \"case_status\", \"case_disposition\",\n",
    "    \"investigation_status\", \"report_completed_i\",\n",
    "    \"date_reported\", \"time_reported\", \"time_police_notified\",\n",
    "    # adjudication / enforcement\n",
    "    \"citation\", \"citation_number\", \"violation_code\", \"ticket_number\", \"arrest_flag\", \"charge_code\",\n",
    "    # medical / EMS / post-crash response\n",
    "    \"ems_run_number\", \"ambulance_id\", \"hospital_name\", \"hospital_code\", \"trauma_center\",\n",
    "    # file/meta columns that shouldnâ€™t be model features\n",
    "    \"created_at\", \"updated_at\", \"loaded_at\", \"ingest_time\", \"etl_batch_id\", \"source_file\", \"row_hash\"\n",
    "}\n",
    "\n",
    "# 2) Regex patterns to catch families of leakage columns (flag only)\n",
    "COMMON_LEAKAGE_PATTERNS = [\n",
    "    # IDs & JSON aggregates\n",
    "    r\"_id_list_json$\", r\"\\b[a-z]+_id\\b\", r\"\\bveh_.*_id\\b\", r\"\\bppl_.*_id\\b\",\n",
    "    # enforcement & adjudication\n",
    "    r\"\\bcitation\", r\"\\bticket\", r\"\\bviolation\", r\"\\barrest\", r\"\\bcharge\",\n",
    "    # post-event admin times/dates\n",
    "    r\"date_.*(reported|notified)\", r\"time_.*(reported|notified)\",\n",
    "    # injuries / severity (likely label leakage for certain outcomes)\n",
    "    r\"\\binjur\", r\"incapacitating|serious|severe|fatal|death|deceased|pronounced\",\n",
    "    # tow-related (post-crash response)\n",
    "    r\"\\btow(ed|ing)?\\b\", r\"vehicle_?_?towed\", r\"num_vehicles_towed\", r\"tow_company|tow_destination\",\n",
    "    # medical/EMS\n",
    "    r\"\\bems\\b|\\bambulance\\b|\\bhosp|\\btrauma|\\ber\\b|\\bmedic\",\n",
    "    # meta/ETL\n",
    "    r\"^etl_\", r\"^ingest_\", r\"^load_\", r\"_ingested$\", r\"_loaded$\"\n",
    "]\n",
    "\n",
    "# 3) Scan columns\n",
    "cols = df.columns.tolist()\n",
    "suspects_exact = [c for c in cols if c in COMMON_LEAKAGE_EXACT]\n",
    "rx = re.compile(\"|\".join(COMMON_LEAKAGE_PATTERNS), flags=re.IGNORECASE)\n",
    "suspects_regex = [c for c in cols if rx.search(c)]\n",
    "\n",
    "# De-dup + remove true target\n",
    "suspects = sorted(set(suspects_exact) | set(suspects_regex))\n",
    "if TARGET in suspects:\n",
    "    suspects.remove(TARGET)\n",
    "\n",
    "if suspects:\n",
    "    print(\"[LEAKAGE] Common leakage/ID/post-event suspects:\")\n",
    "    display(pd.DataFrame({\"suspected_leakage_cols\": suspects}).assign(source=\"common\"))\n",
    "    issues.append((\"LEAKAGE\", f\"Common leakage/ID/post-event suspects: {suspects[:20]}{'...' if len(suspects)>20 else ''}\"))\n",
    "else:\n",
    "    print(\"[LEAKAGE] No common leakage/ID/post-event columns matched.\")\n",
    "\n",
    "# 4) Keeping â€œFORBIDDENâ€ list as a strict subset \n",
    "FORBIDDEN = {\n",
    "    \"report_type\",\"photos_taken_i\",\"statements_taken_i\",\"date_police_notified\",\n",
    "    \"veh_vehicle_id_list_json\",\"ppl_person_id_list_json\",\"location_json\",\"street_name\"\n",
    "}\n",
    "present_forbidden = [c for c in FORBIDDEN if c in df.columns and c != TARGET]\n",
    "if present_forbidden:\n",
    "    issues.append((\"LEAKAGE\", f\"Forbidden (leakage/IDs) present: {present_forbidden}\"))\n",
    "    print(\"[LEAKAGE] Forbidden present:\", present_forbidden)\n",
    "else:\n",
    "    print(\"[LEAKAGE] No forbidden columns detected (strict list).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e956e29",
   "metadata": {},
   "source": [
    "## **10) Derived Time Features Present**\n",
    "\n",
    "Verifies year, month, day_of_week, hour, is_weekend are available; shows date range and latest crash date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = issues if 'issues' in globals() else []\n",
    "expected_time_cols = ['year','month','day_of_week','hour','is_weekend']\n",
    "missing_time = [c for c in expected_time_cols if c not in df.columns]\n",
    "if missing_time:\n",
    "    issues.append((\"TIME\", f\"Missing derived time columns: {missing_time}\"))\n",
    "    print(\"[TIME] Missing:\", missing_time)\n",
    "else:\n",
    "    print(\"[TIME] All derived time columns present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cf559",
   "metadata": {},
   "source": [
    "## **11) Geo Bins & grid_id Consistency**\n",
    "\n",
    "Checks that grid_id numerically matches lat_bin/lng_bin (ignoring formatting) and reports any mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "issues = issues if 'issues' in globals() else []\n",
    "expected_geo = ['lat_bin','lng_bin','grid_id']\n",
    "missing_geo = [c for c in expected_geo if c not in df.columns]\n",
    "if missing_geo:\n",
    "    issues.append((\"GEO\", f\"Missing geo bin columns: {missing_geo}\"))\n",
    "    print(\"[GEO] Missing:\", missing_geo)\n",
    "else:\n",
    "    derived = df['lat_bin'].astype(str).str.strip() + \"_\" + df['lng_bin'].astype(str).str.strip()\n",
    "    mismatch = (df['grid_id'].astype(str).str.strip() != derived).sum()\n",
    "    if mismatch > 0:\n",
    "            print(\"[GEO] grid_id matches lat/lng bins\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b4337",
   "metadata": {},
   "source": [
    "## **12) Categorical cardinality & Outliers**\n",
    "\n",
    "Flags high-cardinality categoricals to compress and runs an IQR-based scan for numeric outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Categorical\n",
    "cat_cols = [c for c in df.columns if df[c].dtype == 'object']\n",
    "cat_card = pd.DataFrame({'column': cat_cols, 'unique_values': [df[c].nunique(dropna=True) for c in cat_cols]}).sort_values('unique_values', ascending=False)\n",
    "display(cat_card.head(20))\n",
    "if (cat_card['unique_values'] > HIGH_CARD_LIMIT).any():\n",
    "    issues.append((\"CARDINALITY\", \"High-cardinality categoricals present\"))\n",
    "\n",
    "# Outliers (IQR)\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "out_rows = []\n",
    "for c in num_cols:\n",
    "    s = df[c].dropna()\n",
    "    if s.empty: \n",
    "        continue\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if not pd.isna(iqr) and iqr != 0:\n",
    "        lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        rate = ((df[c] < lo) | (df[c] > hi)).mean()\n",
    "        rate = float(rate) if not pd.isna(rate) else 0.0\n",
    "        out_rows.append((c, rate))\n",
    "out_df = pd.DataFrame(out_rows, columns=['column','outlier_rate']).sort_values('outlier_rate', ascending=False)\n",
    "display(out_df.head(20))\n",
    "if (out_df['outlier_rate'] > OUTLIER_RATE_WARN).any():\n",
    "    issues.append((\"OUTLIERS\", \"Some numeric columns exceed outlier threshold\"))\n",
    "issues = issues if 'issues' in globals() else []\n",
    "violations = []\n",
    "\n",
    "def _max_over(col, k):\n",
    "    if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "        mx = pd.to_numeric(df[col], errors='coerce').max()\n",
    "        if pd.notna(mx) and mx > k:\n",
    "            violations.append((col, float(mx), k))\n",
    "\n",
    "CAPS = {'veh_count': 5, 'ppl_count': 10, 'injuries_total': 20}\n",
    "for col, cap in CAPS.items():\n",
    "    _max_over(col, cap)\n",
    "\n",
    "age_cols = [c for c in df.columns if 'age' in c and pd.api.types.is_numeric_dtype(df[c])]\n",
    "for c in age_cols:\n",
    "    mx = pd.to_numeric(df[c], errors='coerce').max()\n",
    "    if pd.notna(mx) and mx > 110:\n",
    "        violations.append((c, float(mx), 110))\n",
    "\n",
    "if violations:\n",
    "    issues.append((\"OUTLIERS_DOMAIN\", f\"Domain caps exceeded: {violations[:10]}\"))\n",
    "    print(\"[OUTLIERS_DOMAIN] Exceeded caps:\", violations[:10])\n",
    "else:\n",
    "    print(\"[OUTLIERS_DOMAIN] All domain caps satisfied\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee99a4",
   "metadata": {},
   "source": [
    "## **13) Gold Non-Redundancy (Table-Level Duplicate Check)**\n",
    "\n",
    "Queries DuckDB to confirm no duplicate crash_record_id are present in the Gold table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = issues if 'issues' in globals() else []\n",
    "try:\n",
    "    DB_PATH  # noqa\n",
    "    TABLE_NAME  # noqa\n",
    "except NameError:\n",
    "    print(\"[GOLD] Skipped: DB_PATH or TABLE_NAME is not defined in this notebook.\")\n",
    "else:\n",
    "    try:\n",
    "        import duckdb\n",
    "        with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "            total_rows = con.execute(f'SELECT COUNT(*) FROM \"{TABLE_NAME}\"').fetchone()[0]\n",
    "            distinct_ids = con.execute(f'SELECT COUNT(DISTINCT crash_record_id) FROM \"{TABLE_NAME}\"').fetchone()[0]\n",
    "        if total_rows != distinct_ids:\n",
    "            issues.append((\"GOLD_DUPES\", f'Gold table \"{TABLE_NAME}\" has duplicate crash_record_id rows '\n",
    "                                         f'(total={total_rows}, distinct={distinct_ids})'))\n",
    "            print(f'[GOLD] WARNING: total_rows={total_rows}, distinct_ids={distinct_ids}')\n",
    "        else:\n",
    "            print(f'[GOLD] OK: Unique crash_record_id enforced in {TABLE_NAME}')\n",
    "    except Exception as e:\n",
    "        issues.append((\"GOLD_CHECK_ERROR\", f\"Error while checking Gold table: {e!r}\"))\n",
    "        print(\"[GOLD] Error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882bc4b",
   "metadata": {},
   "source": [
    "## **14) Final findings**\n",
    "\n",
    "Generates a concise narrative QC report summarizing findings and prioritized next actions for modeling readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Narrative QC Report  ===\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def _pct(x):\n",
    "    try:\n",
    "        return f\"{100*x:.1f}%\"\n",
    "    except Exception:\n",
    "        return \"-\"\n",
    "\n",
    "# Safely grab prior-cell variables (with defaults)\n",
    "issues      = locals().get(\"issues\", [])\n",
    "df          = locals().get(\"df\")\n",
    "ID          = locals().get(\"ID\", None)\n",
    "DATE        = locals().get(\"DATE\", None)\n",
    "OUTCOME     = locals().get(\"OUTCOME\", \"<unset>\")\n",
    "TARGET      = locals().get(\"TARGET\", \"<unset>\")\n",
    "miss        = locals().get(\"miss\", pd.DataFrame(columns=[\"column\",\"missing_rate\"]))\n",
    "hi_card     = locals().get(\"hi_card\", pd.DataFrame(columns=[\"column\",\"unique_values\"]))\n",
    "flag_out    = locals().get(\"flag_out\", pd.DataFrame(columns=[\"column\",\"outlier_rate\"]))\n",
    "corr_pairs  = locals().get(\"corr_pairs\", pd.DataFrame(columns=[\"col_a\",\"col_b\",\"pearson_r\"]))\n",
    "leak_cols   = locals().get(\"leak_cols\", [])\n",
    "date_range  = locals().get(\"date_range\", None)      # (min_ts, max_ts)\n",
    "geo_stats   = locals().get(\"geo_stats\", None)       # {\"lat_ok\": float, \"lon_ok\": float}\n",
    "target_info = locals().get(\"target_info\", {})       # {\"name\",\"k\",\"values\",\"interpretation\",\"distribution\",\"class_balance\"}\n",
    "dup_preview = locals().get(\"dup_preview\", None)     # sample duplicate IDs\n",
    "\n",
    "print(f\"# QC Summary â€” {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset\n",
    "# ---------------------------\n",
    "print(\"## Dataset\")\n",
    "if df is None:\n",
    "    raise RuntimeError(\"`df` was not found. Run earlier cells first.\")\n",
    "print(f\"Rows Ã— Cols: {df.shape[0]:,} Ã— {df.shape[1]}\")\n",
    "print(f\"ID column: {ID if (isinstance(ID, str) and ID in df.columns) else 'â€”'}\")\n",
    "print(f\"Date column: {DATE if (isinstance(DATE, str) and DATE in df.columns) else 'â€”'}\")\n",
    "print(f\"Outcome: {OUTCOME}  |  Target: {TARGET}\")\n",
    "latest_crash_date = None\n",
    "if isinstance(DATE, str) and (df is not None) and (DATE in df.columns):\n",
    "    _dt = pd.to_datetime(df[DATE], errors=\"coerce\")\n",
    "    if _dt.notna().any():\n",
    "        latest_crash_date = _dt.max()\n",
    "print(f\"Latest crash date:\", latest_crash_date)\n",
    "\n",
    "# ---------------------------\n",
    "# Target Analysis\n",
    "# ---------------------------\n",
    "print(\"## Target Analysis\")\n",
    "if target_info.get(\"name\"):\n",
    "    print(f\"Target column `{target_info['name']}` with {target_info.get('k','?')} distinct non-null values\")\n",
    "    if target_info.get(\"interpretation\"):\n",
    "        print(f\"Interpreted as: {target_info['interpretation']}\")\n",
    "    if target_info.get(\"values\"):\n",
    "        print(\"Example values:\", target_info[\"values\"])\n",
    "    if isinstance(target_info.get(\"distribution\"), pd.DataFrame):\n",
    "        print(\"\\nTop of target distribution:\")\n",
    "        print(target_info[\"distribution\"].to_string())\n",
    "    cb = target_info.get(\"class_balance\")\n",
    "    if isinstance(cb, dict):\n",
    "        print(\"\\nClass balance (binary):\")\n",
    "        print(f\"- total={cb.get('total','?')}, pos={cb.get('pos','?')}, neg={cb.get('neg','?')}\")\n",
    "        if \"minority_rate\" in cb:\n",
    "            print(f\"- minority_rate={cb['minority_rate']:.3f}, imbalance_ratio={cb.get('imbalance_ratio','?')}\")\n",
    "else:\n",
    "    print(\"Target not found or not analyzed in prior cells.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Schema & Primary Key\n",
    "# ---------------------------\n",
    "print(\"## Schema & Primary Key\")\n",
    "schema_msgs = [m for (t,m) in issues if t in {\"SCHEMA\",\"KEY\"}]\n",
    "if schema_msgs:\n",
    "    for m in schema_msgs:\n",
    "        print(\"-\", m)\n",
    "else:\n",
    "    print(\"Required columns present and primary key looks valid.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Boolean Flags (*_i)\n",
    "# ---------------------------\n",
    "print(\"## Boolean Flags (*_i)\")\n",
    "bool_msgs = [m for (t,m) in issues if t == \"BOOLEANS\"]\n",
    "if bool_msgs:\n",
    "    for m in bool_msgs:\n",
    "        print(\"-\", m)\n",
    "else:\n",
    "    print(\"All *_i flags appear to be normalized to {0,1,NaN} or no *_i flags found.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Duplicates\n",
    "# ---------------------------\n",
    "print(\"## Duplicates\")\n",
    "dup_msgs = [m for (t,m) in issues if t == \"DUPLICATES\"]\n",
    "if dup_msgs:\n",
    "    for m in dup_msgs:\n",
    "        print(\"-\", m)\n",
    "    if dup_preview is not None:\n",
    "        print(\"\\nSample duplicate IDs:\")\n",
    "        print(dup_preview.to_string())\n",
    "else:\n",
    "    print(\"No duplicate issues detected.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Missingness\n",
    "# ---------------------------\n",
    "print(\"## Missingness\")\n",
    "if not miss.empty:\n",
    "    top_miss = miss.head(10).copy()\n",
    "    top_miss[\"missing_rate\"] = (top_miss[\"missing_rate\"]*100).round(1).astype(str) + \"%\"\n",
    "    print(\"Top 10 columns by missing rate:\")\n",
    "    print(top_miss.to_string(index=False))\n",
    "else:\n",
    "    print(\"No missingness table available.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Categorical Cardinality\n",
    "# ---------------------------\n",
    "print(\"## Categorical Cardinality\")\n",
    "if not hi_card.empty:\n",
    "    print(f\"{hi_card.shape[0]} categorical columns exceed the high-cardinality threshold.\")\n",
    "    print(\"Top offenders:\")\n",
    "    print(hi_card.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"No high-cardinality categorical columns flagged.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Numeric Outliers (IQR)\n",
    "# ---------------------------\n",
    "print(\"## Numeric Outliers (IQR)\")\n",
    "if not flag_out.empty:\n",
    "    show = flag_out.head(10).copy()\n",
    "    show[\"outlier_rate\"] = (show[\"outlier_rate\"]*100).round(1).astype(str) + \"%\"\n",
    "    print(f\"{flag_out.shape[0]} numeric columns exceed the outlier threshold.\")\n",
    "    print(show.to_string(index=False))\n",
    "else:\n",
    "    print(\"No numeric columns exceeded the outlier threshold.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Domain Caps (veh/ppl/injuries/age)\n",
    "# ---------------------------\n",
    "print(\"## Domain Caps\")\n",
    "dom_msgs = [m for (t,m) in issues if t == \"OUTLIERS_DOMAIN\"]\n",
    "if dom_msgs:\n",
    "    for m in dom_msgs:\n",
    "        print(\"-\", m)\n",
    "else:\n",
    "    print(\"All domain cap checks passed (veh_count, ppl_count, injuries_total, age bounds).\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Date & Geo\n",
    "# ---------------------------\n",
    "print(\"## Date & Geo\")\n",
    "if isinstance(date_range, tuple) and len(date_range) == 2:\n",
    "    print(\"Date range:\", date_range[0], \"â†’\", date_range[1])\n",
    "else:\n",
    "    print(\"No date column parsed or not available.\")\n",
    "if isinstance(geo_stats, dict) and {\"lat_ok\",\"lon_ok\"} <= geo_stats.keys():\n",
    "    print(f\"Geo bounds OK fraction â€” lat: {_pct(geo_stats['lat_ok'])}, lon: {_pct(geo_stats['lon_ok'])}\")\n",
    "else:\n",
    "    print(\"No lat/lon bounds check or columns not present.\")\n",
    "time_msgs = [m for (t,m) in issues if t == \"TIME\"]\n",
    "if time_msgs:\n",
    "    for m in time_msgs:\n",
    "        print(\"-\", m)\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Leakage\n",
    "# ---------------------------\n",
    "print(\"## Leakage\")\n",
    "if leak_cols:\n",
    "    print(\"Suspected (name-based) leakage columns:\")\n",
    "    print(pd.DataFrame({\"suspected_leakage_cols\": leak_cols}).head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"No suspected leakage columns by name heuristic.\")\n",
    "leak_enforce = [m for (t,m) in issues if t == \"LEAKAGE\"]\n",
    "if leak_enforce:\n",
    "    for m in leak_enforce:\n",
    "        print(\"-\", m)\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Strong Numeric Correlations\n",
    "# ---------------------------\n",
    "# This section only prints if corr_pairs exists and is non-empty.\n",
    "print(\"## Strong Numeric Correlations (|r| â‰¥ 0.7)\")\n",
    "if isinstance(corr_pairs, pd.DataFrame) and not corr_pairs.empty:\n",
    "    print(corr_pairs.head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"None detected above threshold or skipped in this QC.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Gold Layer (Non-Redundancy)\n",
    "# ---------------------------\n",
    "print(\"## Gold Layer\")\n",
    "gold_msgs = [m for (t,m) in issues if t in {\"GOLD_DUPES\",\"GOLD_CHECK_ERROR\"}]\n",
    "if gold_msgs:\n",
    "    for m in gold_msgs:\n",
    "        print(\"-\", m)\n",
    "else:\n",
    "    print(\"Gold table check: no duplicate crash_record_id detected (or check skipped).\")\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# Recommended Next Actions\n",
    "# ---------------------------\n",
    "print(\"## Recommended Next Actions\")\n",
    "recommendations = []\n",
    "for tag, msg in issues:\n",
    "    t = tag.upper()\n",
    "    m = msg.lower()\n",
    "    if t == \"TARGET\" and (\"binary\" in m or \"0/1\" in m or \"encode\" in m):\n",
    "        recommendations.append(\"Map the target to {0,1} and document the mapping used.\")\n",
    "    if t == \"MISSINGNESS\":\n",
    "        recommendations.append(\"Impute or drop columns with high missingness; document choices and thresholds.\")\n",
    "    if t == \"CARDINALITY\":\n",
    "        recommendations.append(\"Reduce high-cardinality categoricals (group rare levels, hashing, or target encoding).\")\n",
    "    if t == \"OUTLIERS\":\n",
    "        recommendations.append(\"Winsorize/clip, transform, or validate data entry for flagged numeric columns.\")\n",
    "    if t == \"OUTLIERS_DOMAIN\":\n",
    "        recommendations.append(\"Enforce domain caps (veh/ppl/injuries/age) or justify exceptions with domain notes.\")\n",
    "    if t == \"DUPLICATES\":\n",
    "        recommendations.append(\"Resolve duplicates with a consistent rule (latest-wins/aggregate) and de-dup the dataset.\")\n",
    "    if t == \"LEAKAGE\":\n",
    "        recommendations.append(\"Drop or justify suspected leakage columns; ensure only report-time features remain.\")\n",
    "    if t == \"BOOLEANS\":\n",
    "        recommendations.append(\"Normalize *_i flags to {0,1} and re-run QC to confirm.\")\n",
    "    if t == \"TIME\":\n",
    "        recommendations.append(\"Add derived time fields (year, month, day_of_week, hour, is_weekend).\")\n",
    "    if t == \"GEO\":\n",
    "        recommendations.append(\"Ensure lat_bin/lng_bin exist and grid_id = f(lat_bin,lng_bin).\")\n",
    "    if t == \"IMBALANCE\":\n",
    "        recommendations.append(\"Handle class imbalance (class weights, resampling, threshold tuning, or focal loss).\")\n",
    "    if t in {\"SCHEMA\",\"KEY\"}:\n",
    "        recommendations.append(\"Satisfy required schema and enforce a unique, non-null primary key.\")\n",
    "\n",
    "if not recommendations:\n",
    "    print(\"- Dataset passes current thresholds; proceed to feature engineering.\")\n",
    "else:\n",
    "    seen = set()\n",
    "    for r in recommendations:\n",
    "        if r not in seen:\n",
    "            seen.add(r)\n",
    "            print(\"-\", r)\n",
    "\n",
    "print(\"\\nâ€” End of QC Report â€”\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8455d",
   "metadata": {},
   "source": [
    "## **ðŸ“˜ Assignment: Convert Your QC Summary into a Professional Data Quality Report**\n",
    "\n",
    "**Goal.** Using the QC summary you just generated, write a **Professional Data Quality Report** for your chosen outcome. \n",
    "\n",
    "\n",
    "### **ðŸ“¦ Deliverables**\n",
    "- **Report (PDF)**\n",
    "\n",
    "\n",
    "### **ðŸ§­ Required Structure (1-2 pages main report)**\n",
    "1) **Executive Summary**  \n",
    "   - One-paragraph verdict on overall data readiness for ML.  \n",
    "\n",
    "2) **Dataset & Target Overview**  \n",
    "   - Rows Ã— columns, ID column, Date column, **Outcome** and **Target** (state the exact column name).  \n",
    "   - Briefly characterize the target (binary/multiclass; class balance: minority %, imbalance ratio).\n",
    "\n",
    "3) **Quality Assessment (evidence-based)**  \n",
    "   Summarize the QC sections **as claims + evidence** (You can cite metrics from your QC output):  \n",
    "   - **Schema & Primary Key:** required fields present? PK unique & non-null?  \n",
    "   - **Missingness:** top missing columns (with %), plan to impute/drop.  \n",
    "   - **Categorical Cardinality:** any high-card columns and your plan to reduce them.  \n",
    "   - **Numeric Outliers (IQR) & Domain Caps:** which columns exceeded caps; justify clips vs. investigation.  \n",
    "   - **Time & Geo:** date range, latest crash date, derived time fields present, consistent with bins.  \n",
    "   - **Leakage:** confirm forbidden columns removed; mention any name-based suspects and actions.  \n",
    "   - **Gold Non-Redundancy:** duplicate key status for the Gold table.\n",
    "\n",
    "4) **Risks & Ethics (2â€“4 bullets)**  \n",
    "   - Potential biases (e.g., missingness patterns by location/time) and impact on model fairness.  \n",
    "   - Consequences of leakage or mis-specified grain.\n",
    "\n",
    "5) **Recommendations & Next Actions**  \n",
    "   - State **go/no-go for modeling** and what must be done before training.\n",
    "\n",
    "\n",
    "### **ðŸ§° Submission Notes**\n",
    "- Keep the main report **1-2 pages**.  \n",
    "- Use consistent notation, and labeled figures/tables.  \n",
    "- If you re-ran QC after fixes, **state what changed**.\n",
    "\n",
    "> **Reminder:** Your report must be **self-contained**. A reviewer should understand the datasetâ€™s fitness for modeling without opening the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
